{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain chromadb fastapi uvicorn pandas PyPDF2 openpyxl requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：读取和分块文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, UnstructuredExcelLoader\n",
    "from langchain.text_splitter import RecursiveJsonSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_split(file_path):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.xlsx', '.xls'):\n",
    "        loader = UnstructuredExcelLoader(file_path, mode = 'elements')\n",
    "    else: \n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 512, chunk_overlap = 64)\n",
    "    \n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# 示例调用\n",
    "chunks = load_and_split(\"./data/电动汽车安全要求.pdf\")\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:100]) # 打印前100字符验证\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步：使用 Ollama 的 bge-m3 模型生成 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "import requests\n",
    "\n",
    "class OllamaEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = 'bge-m3:latest', base_url: str = 'http://localhost:11434'):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            response = requests.post(f\"{self.base_url}/api/embeddings\", json={\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": text\n",
    "            })\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(f\"Error embedding text: {response.text}\")\n",
    "            embeddings.append(response.json()['embedding'])\n",
    "        return embeddings \n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "    \n",
    "# 使用示例\n",
    "embeddings = OllamaEmbeddings(model_name=\"bge-m3:latest\")\n",
    "query_embedding = embeddings.embed_query(\"高压维修断开装置要求有哪些？\")\n",
    "print(len(query_embedding))  # 输出维度，通常是 1024 维\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四步：将向量数据写入 Chroma 数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 初始化 Chroma 向量数据库\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",  # 本地存储路径\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 添加文档到向量库\n",
    "vectorstore.add_documents(documents=chunks)\n",
    "vectorstore.persist()  # 持久化保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import requests\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# 自定义 Ollama Embeddings 类（同上）\n",
    "class OllamaEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"bge-m3:latest\", base_url=\"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            response = requests.post(f\"{self.base_url}/api/embeddings\", json={\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": text\n",
    "            })\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(f\"Error embedding text: {response.text}\")\n",
    "            embeddings.append(response.json()['embedding'])\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# 加载向量数据库\n",
    "embeddings = OllamaEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(query: str, k: int = 5):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    formatted = [{\"content\": doc.page_content, \"score\": score} for doc, score in results]\n",
    "    return {\"results\": formatted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvicorn main:app --reload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
